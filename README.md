âš–ï¸ Agentic RAG on AWS for Legal & Compliance Document Automation
Automating legal document analysis with LangChain Agents, Hugging Face LLMs, FAISS, and AWS serverless stack.
ğŸš€ Overview
This project implements an Agentic Retrieval-Augmented Generation (RAG) system using LangChain Agents and a serverless AWS architecture. The system processes and analyzes legal and compliance documents (CUAD dataset), enabling intelligent querying, summarization, and clause extraction through LLM-driven agents.
âš¡ Outperforms traditional RAG by enabling autonomous reasoning, tool usage, and multi-step decision making.
ğŸ§  Key Features
ğŸ§¾ CUAD-based legal document understanding
ğŸ¤– LangChain Agents for intelligent control flow and reasoning
ğŸ§  Google FLAN-T5 Small via Hugging Face Hub (lightweight & cost-efficient)
ğŸ“š FAISS for fast vector retrieval
ğŸ› ï¸ Serverless compute with AWS Lambda + API Gateway
â˜ï¸ S3 integration to load documents on-demand
ğŸ§© Designed to be extensible with Bedrock and SageMaker
ğŸ”§ Technologies Used
Layer	Tool/Service
Embeddings	sentence-transformers/all-mpnet-base-v2
LLM Inference	FLAN-T5 Small via Hugging Face Hub
Document Store	FAISS (local vector DB)
Orchestration	LangChain Agents
Cloud Compute	AWS Lambda
API Layer	API Gateway
File Storage	AWS S3
Optional Extension	Amazon Bedrock, SageMaker
Why FLAN-T5 Small?
ğŸª¶ Lightweight & serverless-friendly â€“ ideal for Lambda deployment
ğŸš€ Fast inference for prototyping and demos
ğŸ”„ Easily swappable with larger models (Falcon, Claude, Bedrock models) when scaling
